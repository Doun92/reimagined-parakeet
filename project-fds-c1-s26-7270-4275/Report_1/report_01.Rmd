---
title: "FDS Final Project: Report #1"
author: "Daniel Escoval"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: 
  html_document:
    df_print: paged
    code_fold: hide
    toc: true
editor_options: 
  markdown: 
    wrap: sentence
---

```{r include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Activation of the different libraries needed for the project.
library(rvest)
library(dplyr)
library(tidyr)
library(lubridate)
library(purrr)
library(stringr)
library(readr)
library(ggplot2)
library(forcats)
library(infer)
library(tidygeocoder)
library(ggmap)
library(leaflet)
library(knitr)
```

# Introduction
This is the first report of a series of four reports. The goal of each report is to show my skills of

1. Accessing the data
2. Cleaning the data
3. Exploring the data
4. Analysing the data
5. Display any discovery in meaningful reports
6. Write a comprehensive report about

In this report, we will take the data from a fake Real Estate website.
Then, we will explore it, putting an emphasis on some difficulties inherent of these datasets.
Finally, we will display it on a map.
This report should be a good beginning to explore Real Estate data from a real dataset, in a real situation.

# The data

The first step was to actually browse the website to understand how it worls.

Below, a screenshot of the webiste:
<center>
![UI sample from https://epfl-exts.github.io/rental-scrape/](UI_rental_site.png){width=95%}
</center>

While browsing the website, we see that the data is shown is a table view. The UI represents each item in a small cell of a bigger table.The entire data is in a unique page, which means there is no need to go to page 2 and more.
The data provided for the UI is the following:

* Type of item.
* Number of rooms
* Price
* Address
* Superficy
* Floor
* Usable surface
* Availability time

But if we want to scrap this data, we cannot just copy-paste from the UI, we need to get the data from the source code directly. If we inspect the website code, we can see that there is no table, only <div> tags put altogether.
``` {eval=False}
<div class="col-sm">
	<div class="bg-sec-color">
		<div class="bg-sec" style="background-image: url('images/bg-sample6.png');">
			<div class="bg-text">
				<div class="object-type">Apartment</div>
				<div class="rooms">
					<strong>3</strong> Rooms
				</div>
				<div class="price">
					<span class="float-right"><strong>1900</strong> CHF</span>
				</div>
			</div>
		</div>
	</div>
	<div class="sec-info">
		<div class="address">
			<p>Rue de la Terrassière 58, 1207 Genève</p>
		</div>
		<div class="living-space">
			<p>Living space: <strong>63 m2</strong></p>
		</div>
		<div class="floor">
			<p>Floor: <strong>4</strong></p>
		</div>
		<div class="usable-surface">
			<p>Usable surface: <strong>NA</strong></p>
		</div>
	</div>
	<div class="availability sec-info2">
		<p>Availability: <strong>01 Oct 2018</strong></p>
		<p><a href="#"><span class="card-button float-right">See More</span></a></p>
	</div>
	...
</div>
```

Let's first scrap the data and make a table with that.
```{r echo=FALSE,}
# Read HTML page
rental_website <- read_html("https://epfl-exts.github.io/rental-scrape/")
# Create a list of all items.
list_items <- rental_website %>%
  html_elements(".col-sm")

# Now we can create 8 lists from each css tag.

location<- list_items %>%
  html_nodes(css = ".address") %>%
  html_text() %>%
  na_if('NA')

price<- list_items %>%
  html_nodes(css = ".price strong") %>%
  html_text() %>%
  na_if('NA')

currency <- list_items %>%
  html_nodes(css = ".price") %>%
  html_text() %>%
  na_if('NA')

object_type <- list_items %>%
  html_nodes(css = ".object-type") %>%
  html_text() %>%
  na_if('NA')

living_space <- list_items %>%
  html_nodes(css = ".living-space strong") %>%
  html_text() %>%
  na_if('NA')

floor <- list_items %>%
  html_nodes(css = ".floor strong") %>%
  html_text() %>%
  na_if('NA')

availability <- list_items %>%
  html_nodes(css = ".availability strong") %>%
  html_text() %>%
  na_if('NA')

usable_surface <- list_items %>%
  html_nodes(css = ".usable-surface strong") %>%
  html_text() %>%
  na_if('NA')

rooms <- list_items %>%
  html_nodes(css = ".rooms strong") %>%
  html_text() %>%
  na_if('NA')

# Put all together
appartments_df <- dplyr::bind_cols(
  location = location,
  price = price,
  currency = currency,
  object_type = object_type,
  rooms = rooms,
  living_space = living_space,
  floor = floor,
  availability = availability,
  usable_surface = usable_surface
)

glimpse(appartments_df)
```
We do have the 612 rows and 9 columns as expected by the sample data.
But, if we compare both, we see we have some with the data not displayed exactly like in the sample.

```{r message=FALSE, warning=FALSE}
sample_data <- read_csv("sample_scraped_rental.csv")
sample_data
```

Let's explore each column one after another.

The first column is the address of the rental good.
First. we check if we have any NA row in the column.
```{r}
appartments_df %>%
  select(location) %>%
  filter(is.na(location) == TRUE)
```

Now we can focus on the price row.
Prices are discrete data and should be seen as numerics.
```{r}
appartments_df <- appartments_df %>%
  mutate_if(is.numeric,as.numeric)

appartments_df %>%
  head(5)
```
While checking the appartments_df, it seems that the price column is still a character column, not numeric.

A quick check like the one below show us that some prices are, in fact not numbers.
```{r}
appartments_df %>%
  filter(str_detect(price, '\\D'))
```
This mean, we have to accept that and keep it in mind if we try to compare prices by removing these rows.

Now let's focus on the currency column.
As we see the data is displayed in a strange way (\n\t \t\t\n\t \t\t1900 CHF\n \t\t)
```{r}
appartments_df %>%
  select(currency) %>%
  head(5)
```
We found that the easiest way it to extract the currency, as there are only two different possibilities for the data CHF or NA.

```{r}
appartments_df <- appartments_df %>%
  mutate(currency = str_extract(currency, regex("CHF|NA"))) %>%
  mutate(currency = na_if(currency, "NA"))

appartments_df
```

We also need to change the florr column. For now we only have a number, but in the sample csv file, we see that it is a awaiting for a string.
Instead of 4, I should see 4. Floor.
```{r}
appartments_df <- appartments_df %>%
  mutate(floor =  paste(floor, ". Floor", sep = "")) %>%
  mutate(floor = na_if(floor, 'NA. Floor')) %>%
  mutate(floor = str_replace(floor, "Underground. Floor", "Underground"))

appartments_df
```

Finally, we can take care of the last column that is not made of strings.
First we only tried to change the strings to date with the as.Date function and the correct format.
We got an error and then returned to inspect the column.
Note that I had issue with the conversion from string to Date because of the language of my computer.
I had to use Sys.setlocale("LC_TIME", "English") in the console to convert to English.

For now, the best solution is to keep both the string and the dates. 
The solution below was the most elegant we found.
```{r}
appartments_df <- appartments_df %>%
  mutate( availability = if_else(
      availability == "Immediately", 
      "Immediately",
      as.character(as.Date(availability, format="%d %b %Y"))
    )
  )

appartments_df
```

Finally, the table looks like the sample.
An important note here is on the datatype of each column. In fact, each column is a string because there is always a value that impeach the column to be another data type.

To finalise the cleaning, let's check the ranges of the numerical variables. Even if we said right before there is no numeric variable, we can consider price, rooms and superficy as numerical variables, after some changes.

First, let's get the range of prices:
```{r}
appartments_df %>%
  select(price, currency) %>%
  filter(price != "Price on request")%>%
  mutate(price = as.numeric(price)) %>%
  drop_na(price) %>%
  summarise(min_col =min(price),max_col =max(price))
```
We see that the price range is from 30 CHF to 22800 CHF.

As of the Exploratory exercise, it seems that we also should see the living space a numbers. The only thing that impeach us to do so is the unite written at the end. Let's see the range of the living space.
```{r}
appartments_df %>%
  select(living_space) %>%
  drop_na() %>%
  mutate(living_space = str_extract(living_space, regex("\\d+"))) %>%
  mutate(living_space = as.numeric(living_space)) %>%
  summarise(min_col =min(living_space),max_col =max(living_space))
```

The 1 $m^2$ surprised me, so I checked the data:
```{r}
appartments_df %>%
  filter(living_space == "1 m2")
```
We also found it in the UI:
<center>
![The 1m2 appartment](1m_appart.png){width=50%}
</center>
This is an anomaly in the data and should be filterred out during the analysis of the data.

Finally, we can also explore the range of rooms.
```{r}
appartments_df %>%
  select(rooms) %>%
  mutate(rooms = as.numeric(rooms)) %>%
  drop_na() %>%
  summarise(min_col = min(rooms), max_col = max(rooms))
```
So, we have items from 1 room to 13 rooms.
In the opposite of the superficy, it is very likely the Real Estate company suggest 1 room appartments, like studios.
We cannot deduct anything from these min/max, only we can be prepared to see a big diversity in the data from a studio to a big family house.

# Exploration

## Price vs Living space
The first question is about price over superficy.
Let's build a scatter plot that shows the relation between superficy and price.

```{r}
appartments_df_with_int <- appartments_df %>%
  select(price, living_space, object_type) %>%
  filter(price != "Price on request")%>%
  mutate(living_space = str_extract(living_space, regex("\\d+"))) %>%
  mutate(price = as.numeric(price)) %>%
  drop_na() %>%
  mutate(living_space = as.numeric(living_space))

interesting_objects <- appartments_df_with_int %>%
  select(price, living_space, object_type) %>%
  filter(price != "Price on request")%>%
  mutate(living_space = str_extract(living_space, regex("\\d+"))) %>%
  mutate(price = as.numeric(price)) %>%
  drop_na() %>%
  mutate(living_space = as.numeric(living_space))%>%
  filter(object_type %in% c("Hobby room", "Apartment", "Studio", "Villa", "Duplex"))

appartments_df_with_int %>%
  ggplot(mapping = aes(x = living_space,
                     y = price))+
  geom_point() +
  geom_point(data = interesting_objects,
             aes(x = living_space,
                     y = price,
                     color=object_type))+
  geom_smooth(method=lm, se=FALSE) +
  labs(title="Relationship between prices and living space",
       caption="Data from https://epfl-exts.github.io/rental-scrape",
       x="Superficy in m2", 
       y="Price in CHF",
       color = "Type of object")
```
We added to the plot colors to get the type of object to rent, and a regression line to show the correlation between the superficy and the price. 

About the colors, we chose to show the 5 most relevant object, the black dots are all the other kind of objects.

We can see that there is a positive relationship between living space and prices. I means that more living space is given and more expensive the rental is.

The relationship is quiet linear, we have a few exceptions, like the 1$m^2$ which costs 2180 and is easily visible.

We also can see a small dispersion in the most common data (between ~>10$m^2$ to ~120$m^2$), then it seems to be way more dispersed. there is also less data after 200$m^2$.

We can arrive to such a conclusion that, indeed, there is a big probability that bigger the living space, more expensive it is, but this rule seems to be less rigid in bigger spaces (more than 200$m^2$).

Adding a third variable as the type of object, we can also see that the Hobby room objects are smaller (even if one is bigger than studios and apartments) and they are way cheaper than the other objects, which may influence the regression line.

Finally, we see that the biggest item is also the most expensive, which confirms the trend analysis we do. We can also be surprised to see that the most expensive item is not a villa or an individual house, but an apartment.

## Properties by postcode
Now we have the cost per superficy, it would also be interesting to see where the agency is the most present.
Let's build now a bar plot showing the number of properties by postcode.
```{r fig.height=10}
appartments_df %>%
  select(location) %>%
  mutate(code = str_extract(location, regex("\\d{4}"))) %>%
  group_by(code) %>%
  summarise(n_of_properties = n()) %>%
  ungroup %>%
  arrange(desc(n_of_properties)) %>%
  ggplot(aes(x = fct_reorder(code, n_of_properties), y = n_of_properties)) +
  geom_col(width = 0.7) +
  coord_flip()+
  #scale_x_discrete(guide = guide_axis(n.dodge=3))
  #guides(x = guide_axis(angle = 90)) +
  labs(
    title="Number of properties per postal code",
    caption="Data from https://epfl-exts.github.io/rental-scrape",
    x="Postal code",
    y="Number of properties"
    )
```
We can see that the agency is more active in the area of 1206 and 1205.
These are two residential district in Geneva. These district, at least the 1206 are known to be high-class districts.

Now we see that some districts are well represented.
Let's draw a plot separating the different districts.

```{r}
price_sur_loc_fl <- appartments_df %>%
  select(price, living_space,location,floor) %>%
  filter(price != "Price on request")%>%
  mutate(living_space = str_extract(living_space, regex("\\d+"))) %>%
  mutate(price = as.numeric(price)) %>%
  drop_na() %>%
  mutate(living_space = as.numeric(living_space)) %>%
  mutate(code = str_extract(location, regex("\\d{4}"))) %>%
  mutate(floor = str_replace(floor, "Underground", "0")) %>%
  mutate(floor = str_extract(floor, regex("\\d+"))) %>%
  mutate(floor = as.numeric(floor)) %>%
  filter(between(floor, 0, 6)) %>%
  group_by(code) %>%
  filter(n()>10)

price_sur_loc_fl %>%
  ggplot(mapping = aes(x = living_space,
                     y = price,
                     color=floor))+
  geom_point() + 
  facet_wrap(vars(code))+
  labs(
    title="Price of flats over living space",
    subtitle = "Separated by postal code using facets and color for floors",
    caption="Data from https://epfl-exts.github.io/rental-scrape",
    x="Superficy in m2",
    y="Price in CHF",
    color = "Floor"
    )
```
For this plot, we made a choice, we wanted to show only the districts with more than 10 items.
This permits to first see that some districts have some limited offers, and it seems that the price range also depends of the district. If we take the district 1205 for example, we see that the superficy does not exceeds 150$m^2$. We can also see that the district 1206, which is the most represented has a wide variety of apartments and it seems distributed in the diagonal of perfect correlation between superficy and price.

We can also switch the color with the facets.
```{r}
price_sur_loc_fl %>%
  ggplot(mapping = aes(x = living_space,
                     y = price,
                     color=code))+
  geom_point() + 
  facet_wrap(vars(floor))+
  labs(
    title="Price of flats over living space",
    subtitle = "Separated by floor using facets and color for district",
    caption="Data from https://epfl-exts.github.io/rental-scrape",
    x="Superficy in m2",
    y="Price in CHF",
    color = "District"
    )
```

Having a grid by floor permits to see that they also follow the same diagonal, bigger the superficy, bigger the price.

```{r echo=FALSE}
mean_expensive <- price_sur_loc_fl %>%
  group_by(code) %>%
  reframe(code, most_exp = max(price), least_exp = min(price), mean_exp = round(mean(price),2)) %>%
  distinct(code, .keep_all = TRUE) %>%
  arrange(mean_exp)
```
The least expensive postal code according to the meaning of all the data we have is `r mean_expensive%>%arrange(mean_exp)%>%head(1)%>%select(code)` with a mean price of `r mean_expensive%>%arrange(mean_exp)%>%head(1)%>%select(mean_exp)` CHF.

The most expensive postal code according to the meaning of all the data we have is `r mean_expensive%>%arrange(desc(mean_exp))%>%head(1)%>%select(code)` with a mean price of `r mean_expensive%>%arrange(desc(mean_exp))%>%head(1)%>%select(mean_exp)` CHF.

# Analysis

## Visible addresses vs addresses on demand
We explored the data, and got some conclusions. Now we will do some analysis in the data.
Our analysis will focus on two types of addresses, indeed some addresses are name and can be easily find on a map, other are on request. Note that the website represents English and French data, so we will need to search for addresses on request, and <i>sur demande</i>.


```{r}
on_demand_address <- appartments_df %>%
  filter(price != "Price on request")%>%
  mutate(living_space = str_extract(living_space, regex("\\d+"))) %>%
  drop_na(living_space) %>%
  mutate(price = as.numeric(price)) %>%
  mutate(living_space = as.numeric(living_space)) %>%
  mutate(on_request = str_extract(location, regex("Address on request|Sur demande"))) %>%
  mutate(on_request = str_detect(on_request, regex("Address on request|Sur demande"))) %>%
  mutate(floor = str_replace(floor, "Underground", "0")) %>%
  mutate(floor = str_extract(floor, regex("\\d+"))) %>%
  mutate(floor = as.numeric(floor)) %>%
  mutate(price_per_square_meter = price/living_space) %>%
  mutate(on_request = ifelse(on_request,'on request','available')) %>%
  mutate(on_request = replace_na(on_request,'available')) %>%
  # We filter out some extreme data
  filter(living_space>1)%>%
  filter(price_per_square_meter<75) %>%
  filter(price_per_square_meter>10)

```

Let's see if we can see any trends for listings with addresses only available on demand.


To answer this question, we first need to create a variable where we are able to easily segregate on demand addresses from the ones not on demand.

Once we have the variable, we can draw the first plot which shows a different distribution between these 2 variables.
```{r}
on_demand_address %>%
  ggplot(aes(x=on_request, y=price)) +
  geom_violin(fill="gray")+
  geom_boxplot(width=0.1) +
  labs(
    title="Comparison of price trends between on demand addresses and written ones.",
    caption="The red dot represents the mean value.
    Data from https://epfl-exts.github.io/rental-scrape",
    x="Status of the address",
    y="Price in CHF"
    )+
  stat_summary(fun="mean", colour = "red", size = 0.25)
```

The 2 plots are very different. Let's analyse them one by one, then conclude.
Both plots represent a distribution of items based on their price and if their address is available on the agency website or not.


The first plot represents the <i>available addresses</i>. We see that the median and the mean values are quite close, as the big majority of prices are a bit under 2500 CHF.
If the mean is not superposed to the median it is mostly because of some extreme values that make the mean bigger than the median.
Spreaking about extreme values, if we compare these with the <i>on request addresses</i>, we see that they are less present, but bigger.

The second plot represents <i>on request addresses</i>
The mean and the median are very close, but not totally superposed. This means that the distribution is better distributed around the mean, but not totally. In fact, some extreme values make the mean bigger than the median.

We can conclude that if we search to rent an item, we have bigger chances to get a cheaper one if looking only at available addresses, but that the prices are more similar in the unshown addresses.

We can also compare the surface between on demand addresses and written addresses.
Let's plot it in a violin plot which permits to have a better understanding of the number of items.

```{r}
on_demand_address %>%
  # Basic violin plot
  ggplot(aes(x=on_request, y=living_space)) + 
  geom_violin(fill="gray")+
  geom_boxplot(width=0.1) +
  labs(
    title="Comparison of surface trends between on demand addresses and written ones.",
    caption="The red dot represents the mean value.
    Data from https://epfl-exts.github.io/rental-scrape",
    x="Status of the address",
    y="Superficy"
    )+
  stat_summary(fun="mean", colour = "red", size = 0.25)
```
The 2 plots are very different. Let's analyse them one by one, then conclude.

The first plot represents the distribution of available apartments based on their superficy.
We see that the median is close to 100$m^2$ with a majority of apartments a bit below this value.
In the opposite, the mean value is higher, as there are some extreme values with more than 500$m^2$ of superficy.

The second violin plot shows the distribution of <i> on request </i> addresses.
The median and the mean re superposed, which means that the data is equally distributed around it mean. In fact, we see that the majority of items is at mean / average value.
Also, the mean / median value are closer to 200$m^2$ than 100$m^2$.
The maximum values are also less extreme than the ones at the <i>available addresses </i>

To conclude we can deduct that we have more chances to have a bigger item if we go to one without its addresses showed in the agency website.

The violin charts show a lot of data, but it is more secure having a table summarising group size, median, average, standard-deviation, minimum and maximum of the variable *price per square-meter* (expressed in CHF/$m^2$), by address status (on request or available).

```{r}
on_demand_address_stats <- on_demand_address %>%
  mutate(price_per_square_meter = price/living_space) %>%
  group_by(on_request) %>%
  summarise(
    group_size = n(),
    median = median(price_per_square_meter),
    average = mean(price_per_square_meter),
    standard_deviation = sd(price_per_square_meter),
    minimum = min(price_per_square_meter),
    maximum = max(price_per_square_meter)
  )
```

Let's analyse this table column by column. First, the group size column is already interesting. It shows that 25% of the overall data has no location, but has to be requested. 
Then, if we compare both medians, we see that the numbers are very close. It means that for both there are 50% of locations cheaper than 33CHF/$m^2$ and 50% of location more expensive. 
The "average" column precises the distinction between the <i>on request</i> and the available addresses. In fact, the average of price per $m^2$ for the on request location is very close from the median which permits to already predict a small distribution. On the opposite, the bigger average in the available addresses shows that an extreme value is pushing the numbers up.
This distribution difference is even better pictured by the standard deviation column. In fact, closer the standard deviation is from 0 and smaller the distribution is. We see that the standard deviation with a value of 110 shows a big dispersion of the data. We also need to recall that the <i> available addresses <i> are represented 3 times more, than the <i>on request </i>.
Finally, the minimal and maximal value kind of confirm our assumptions. We see that the maximum value in the on demand locations is ~ 4 times bigger than the minimum, but that there should not be a  lot of these values. On the opposite, in the available locations row, we that the minimum and maximum are very different and we can expect a dataset with a big spread.

We can also show this data in a histogram chart.
```{r}
on_demand_address %>%
  ggplot(mapping = aes(x = price_per_square_meter, fill = on_request)) +
  geom_histogram() +
  labs(
    title="Distribution of number of items per price per square meter",
    caption="Data from https://epfl-exts.github.io/rental-scrape",
    x="Count of apartments",
    y="Price per square meter in CHF"
  )

```
We are happy to get a shape close to the Bayesian bell, which means the data is well enough distributed.


We can suggest the following null hypothesis: Both on request and available locations have the same price/$m^2$ mean. Here is the t-test
```{r}
t_test_pm <-  on_demand_address %>%
  t_test(price_per_square_meter ~ on_request,
         order = c("on request", "available"))

t_test_pm
```
What we see is that 95% of the population will fall between +5% and -17% of the standard deviation. The p-value is 0.28, which means we have 28% of risk that the null theory is correct, which means that the mean between <i>on request</i> and </i>available addresses</i> would be the same.

Let's do the same with the variable price.
```{r}
on_demand_address %>%
  group_by(on_request) %>%
  summarise(
    group_size = n(),
    median = median(price),
    average = mean(price),
    standard_deviation = sd(price),
    minimum = min(price),
    maximum = max(price)
  )
```
We already saw the difference between the median and the average on part 5. This table only confirms this. On average, the on request addresses are more expensive. We also see that both have huge standard deviation, which means that the data is spear far from the median. We have such confirmation by seeing both minimum and maximum data.


We can suggest a null hypothesis like: Both on request and available locations have a mean price.
```{r}
t_test_p <-  on_demand_address %>%
  t_test(price ~ on_request,
         order = c("on request", "available"))

t_test_p
```
We see that the null hypothesis is not negate by the data.


Compare the results of the analysis carried on the *price per square-meter* with those for the *price variables*.
Are there differences between the results?
Write down your interpretation and a general conclusion.

What we see is that the prices are very likely to be the same in "on request" and "available" location. But, the price per $m^2$ are more likely to be different. But, if we compare both median and means, it seems that on request addresses have a better price per $m^2$, which means that they are bigger for the same value.
We can also see that available addresses have extreme numbers in both minimum and maximum. This may show a need to name bix and expensive items, or smaller ones, so the customer would not be surprised. In the other side, the "on demand" ones have a more similar shape, which may mean that they are flats from particular who do not want to disclose their address everywhere. Price per $m^2$, raw price and the code may be enough to locate the on request addresses.

## Mapping the some addresses
Finally, we will convert a subset of addresses and put it in a map.

```{r}
geo_localisation <- on_demand_address %>%
  filter(on_request == "available") %>%
  head(30)

geo_localisation <- geo_localisation %>%
  tidygeocoder::geocode(location, method = 'osm', lat = latitude , long = longitude) %>%
  select(location, latitude, longitude)


register_stadiamaps("5f54e0a3-5721-407e-b249-a9a9d700d82e", write=FALSE)
stadiamaps_key()
has_stadiamaps_key()

geo_localisation %>%
  leaflet() %>% 
  addTiles() %>% 
  addMarkers(lng=~longitude,lat=~latitude,
             label=~as.character(location),
             popup=~location)
```

# Conclusion
This report shows a real way to analyse a real estate website.
We can compare prices, locations and superficy and display it in meaningful plots.

# Sources
## Help for coding
https://ggplot2.tidyverse.org/index.html
http://www.sthda.com/english/wiki/ggplot2-scatter-plots-quick-start-guide-r-software-and-data-visualization
http://www.sthda.com/english/wiki/ggplot2-violin-plot-quick-start-guide-r-software-and-data-visualization
http://www.sthda.com/english/wiki/ggplot2-histogram-plot-quick-start-guide-r-software-and-data-visualization
https://www.wallstreetmojo.com/regression-line/


## Help for analysis
https://fr.wikipedia.org/wiki/Champel
